setwd("~/Udemy Courses/Machine-Learning-A-Z-New/Machine Learning A-Z New/Part 9 - Dimensionality Reduction/Section 43 - Principal Component Analysis (PCA)")
dataset = read.csv('Wine.csv')
View(dataset)
library(caTools)
set.seed(123)
split = sample.split(dataset$Customer_Segment, SplitRatio = 0.8)
Training_set = subset(dataset, split == TRUE)
Test_set = subset(dataset, split == FALSE)
dataset = scale(dataset)
View(dataset)
View(dataset)
dataset = read.csv('Wine.csv')
## scale
dataset[,1:13] = scale(dataset[,1:13])
View(dataset)
dataset = read.csv('Wine.csv')
## scale
dataset[,1:13] = scale(dataset[,1:13])
## train/test split
library(caTools)
set.seed(123)
split = sample.split(dataset$Customer_Segment, SplitRatio = 0.8)
Training_set = subset(dataset, split == TRUE)
Test_set = subset(dataset, split == FALSE)
View(Training_set)
View(dataset)
classifier = glm(formula = Customer_Segment ~ .,
data = Training_set)
summary(classifier)
classifier = glm(formula = Customer_Segment ~ .,
family = binomial,
data = Training_set)
dataset = read.csv('Wine.csv')
## train/test split
library(caTools)
set.seed(123)
split = sample.split(dataset$Customer_Segment, SplitRatio = 0.8)
Training_set = subset(dataset, split == TRUE)
Test_set = subset(dataset, split == FALSE)
## Feature scaling
Training_set[,1:13] = scale(Training_set[,1:13])
Test_set[,1:13] = scale(Test_set[,1:13])
classifier = glm(formula = Customer_Segment ~ .,
family = binomial,
data = Training_set)
## Applying PCA
install.packages('caret')
## Applying PCA
install.packages('caret')
## Applying PCA
#install.packages('caret')
#install.packages("caret", dependencies = c("Depends", "Suggests"))
library(caret)
pca = preProcess(x = Training_set,
method = 'pca',
pcaComp = 2)
Training = predict(pca, Training_set[,1:13])
predict(pca, Training_set[,1:13])
pca = preProcess(x = Training_set[,1:13],
method = 'pca',
pcaComp = 2)
Training = predict(pca, Training_set[,1:13])
View(Training)
pca = preProcess(x = Training_set[,1:13],
method = 'pca',
pcaComp = 2)
Training_set = predict(pca, Training_set)
Test_set = predict(pca, Test_set)
dataset = read.csv('Wine.csv')
## train/test split
library(caTools)
set.seed(123)
split = sample.split(dataset$Customer_Segment, SplitRatio = 0.8)
Training_set = subset(dataset, split == TRUE)
Test_set = subset(dataset, split == FALSE)
## Feature scaling
Training_set[,1:13] = scale(Training_set[,1:13])
Test_set[,1:13] = scale(Test_set[,1:13])
## Applying PCA
#install.packages('caret')
#install.packages("caret", dependencies = c("Depends", "Suggests"))
library(caret)
library(e1071)
pca = preProcess(x = Training_set[,1:13],
method = 'pca',
pcaComp = 2)
Training_set = predict(pca, Training_set)
Test_set = predict(pca, Test_set)
View(Training_set)
Training_set = Training_set[c(2,3,1)]
View(Training_set)
Test_set = Test_set[c(2,3,1)]
classifier = glm(formula = Customer_Segment ~ .,
family = binomial,
data = Training_set)
classifier = glm(formula = Customer_Segment ~ .,
family = gaussian,
data = Training_set)
summary(classifier)
y_pred_prob = predict(classifier, type = 'response', newdata = test_set[-14])
dataset = read.csv('Wine.csv')
## train/test split
library(caTools)
set.seed(123)
split = sample.split(dataset$Customer_Segment, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
## Feature scaling
training_set[,1:13] = scale(training_set[,1:13])
test_set[,1:13] = scale(test_set[,1:13])
training_set = predict(pca, training_set)
training_set = training_set[c(2,3,1)]
test_set = predict(pca, test_set)
test_set = test_set[c(2,3,1)]
## logistic regression
classifier = glm(formula = Customer_Segment ~ .,
family = gaussian,
data = training_set)
y_pred_prob = predict(classifier, type = 'response', newdata = test_set[-14])
y_pred_prob
y_pred_prob = predict(classifier,  newdata = test_set[-14])
y_pred_prob
classifier = svm(formula = Customer_Segment ~ .,
data = training_set,
type = 'C-classification',
kernel = 'linear')
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-14])
y_pred
cm = table(test_set[,14],y_pred)
cm = table(test_set[14],y_pred)
test_set[14]
test_set[-14]
test_set[-3]
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-3])
### making the confusion matrix
cm = table(test_set[3],y_pred)
y_pred
test_set[3]
as.array(y_pred)
y_pred
cm = table(as.array(test_set[3]),y_pred)
cm = table(test_set[3],y_pred)
cm = table(test_set[,3],y_pred)
cm
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Logistic Regression (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 2, 'deepskyblue', ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 2, 'blue4', ifelse(set[, 3] == 1, 'green4', 'red3')))
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Logistic Regression (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 2, 'deepskyblue', ifelse(y_grid == 1, 'springgreen3', 'tomato')))
points(set, pch = 21, bg = ifelse(set[, 3] == 2, 'blue4', ifelse(set[, 3] == 1, 'green4', 'red3')))
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'Logistic Regression (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 2, 'deepskyblue', ifelse(y_grid == 1, 'springgreen3', 'tomato')))
points(set, pch = 21, bg = ifelse(set[, 3] == 2, 'blue4', ifelse(set[, 3] == 1, 'green4', 'red3')))
y_grid
source('~/Udemy Courses/Machine-Learning-A-Z-New/Machine Learning A-Z New/Part 9 - Dimensionality Reduction/Section 43 - Principal Component Analysis (PCA)/main.R')
dataset = read.csv('Wine.csv')
## train/test split
library(caTools)
set.seed(123)
split = sample.split(dataset$Customer_Segment, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
## Feature scaling
training_set[,1:13] = scale(training_set[,1:13])
test_set[,1:13] = scale(test_set[,1:13])
## Applying PCA
#install.packages('caret')
#install.packages("caret", dependencies = c("Depends", "Suggests"))
library(caret)
pca = preProcess(x = training_set[,1:13],
method = 'pca',
pcaComp = 2)
training_set = predict(pca, training_set)
training_set = training_set[c(2,3,1)]
test_set = predict(pca, test_set)
test_set = test_set[c(2,3,1)]
# Fitting SVM to the Training set
# install.packages('e1071')
library(e1071)
classifier = svm(formula = Customer_Segment ~ .,
data = training_set,
type = 'C-classification',
kernel = 'linear')
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-3])
### making the confusion matrix
cm = table(test_set[,3],y_pred)
cm
# Visualising the Training set results
# install.packages('ElemStatLearn')
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'Logistic Regression (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 2, 'deepskyblue', ifelse(y_grid == 1, 'springgreen3', 'tomato')))
points(set, pch = 21, bg = ifelse(set[, 3] == 2, 'blue4', ifelse(set[, 3] == 1, 'green4', 'red3')))
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('PC1', 'PC2')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'SVM (Training set)',
xlab = 'PC1', ylab = 'PC2',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 2, 'deepskyblue', ifelse(y_grid == 1, 'springgreen3', 'tomato')))
points(set, pch = 21, bg = ifelse(set[, 3] == 2, 'blue3', ifelse(set[, 3] == 1, 'green4', 'red3')))
cm
