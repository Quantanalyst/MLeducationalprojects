geom_point(y = dataset$Salary, x = dataset$Level)
View(dataset)
ggplot() +
geom_point(y = Salary, x = Level)
ggplot() +
geom_point(x = Level, y = Salary)
ggplot() +
geom_point(x = dataset$Level, y = dataset$Salary)
ggplot() +
geom_point(x = dataset$Level, y = dataset$Salary)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary))
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red')
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red') +
geom_line(aes(x = dataset$Level, y = predict(regressor,dataset)))
### visualization by ggplot2
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red') +
geom_line(aes(x = dataset$Level, y = predict(regressor,dataset))) +
ggtitle('Salary vs. Level') +
xlab('Level') +
ylab('Salary ($)')
setwd("~/Udemy Courses/Machine-Learning-A-Z-New/Machine Learning A-Z New/Part 2 - Regression/Section 7 - Support Vector Regression (SVR)")
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[1:2]
View(dataset)
View(dataset)
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[2:3]
View(dataset)
install.packages('e1071')
library(e1071)
svregressor = svm(Salary ~.,
data = dataset,
kernel = 'radial',
type = 'eps-regression')
summary(svregressor)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red')
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red') +
geom_line(aes(x = dataset$Level, y = predict(svregressor,dataset$Level)), color = 'blue')
predict(svregressor, dataset$Level)
y_pred = predict(svregressor,dataset$Level)
svregressor = svm(formula = Salary ~.,
data = dataset,
kernel = 'radial',
type = 'eps-regression')
regressor = svm(formula = Salary ~.,
data = dataset,
kernel = 'radial',
type = 'eps-regression')
### see the prediction for an employee with level 6 and 2 years of experience (Level = 6.5)
y_pred = predict(regressor,data.frame(Level = 6.5))
y_pred
### visualization
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red') +
geom_line(aes(x = dataset$Level, y = predict(regressor, dataset)), color = 'blue') +
ggtitle('Salary vs. Level') +
xlab('Level') +
ylab('Salary($)')
setwd("~/Udemy Courses/Machine-Learning-A-Z-New/Machine Learning A-Z New/Part 2 - Regression/Section 8 - Decision Tree Regression")
dataset = read.csv('Position_Salaries')
dataset = read.csv('Position_Salaries.csv')
View(dataset)
View(dataset)
dataset = dataset[2:3]
View(dataset)
View(dataset)
install.packages('rpart')
library(rpart)
View(dataset)
regressor = rpart(formula = Salary ~.,
data = dataset)
summary(regressor)
y_pred = predict(regressor,dataset)
y_pred
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red')
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red') +
geom_line(aes(x = dataset$Level, y=y_pred), color = 'blue') +
ggtitle('Salary vs. Level') +
xlab('Level') +
ylab('Salary')
regressor = rpart(formula = Salary ~.,
data = dataset,
control = rpart.control(minsplit = 1))
summary(regressor)
y_pred = predict(regressor,dataset)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red') +
geom_line(aes(x = dataset$Level, y=y_pred), color = 'blue') +
ggtitle('Salary vs. Level') +
xlab('Level') +
ylab('Salary')
x_grid = seq(min(dataset$Level),max(dataset$Level),0.1)
x_grid = seq(min(dataset$Level),max(dataset$Level),0.01)
y_pred = predict(regressor,x_grid)
y_pred = predict(regressor,data.frame(Level = x_grid))
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red') +
geom_line(aes(x = dataset$Level, y=y_pred), color = 'blue') +
ggtitle('Salary vs. Level') +
xlab('Level') +
ylab('Salary')
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red') +
geom_line(aes(x = x_grid, y=y_pred), color = 'blue') +
ggtitle('Salary vs. Level') +
xlab('Level') +
ylab('Salary')
setwd("~/Udemy Courses/Machine-Learning-A-Z-New/Machine Learning A-Z New/Part 2 - Regression/Section 9 - Random Forest Regression")
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[2:3]
install.packages('RandomForest')
install.packages('randomForest')
library(randomForest)
regressor = randomForest(x = dataset$Level,
y = dataset$Salary,
ntree = 100)
regressor = randomForest(x = dataset[1],
y = dataset[2],
ntree = 100)
View(dataset)
regressor = randomForest(x = dataset[1],
y = dataset[2])
regressor = randomForest(x = dataset[1],
y = dataset$Salary,
ntree = 10)
y_pred = regressor.predict(x = dataset[1])
regressor = randomForest(x = dataset[1],
y = dataset$Salary,
ntree = 10)
y_pred = regressor.predict(x = dataset[1])
y_pred = predict(regressor, dataset$Level)
y_pred = predict(regressor, dataset[1])
y_pred
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red')
x_grid = seq(min(dataset$Level), max(dataset$Salary), 0.01)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red') +
geom_line(aes(x = x_grid, y = predict(regressor, x_grid), color = 'blue'))
y_pred = predict(regressor, x_grid)
x_grid = seq(min(dataset$Level), max(dataset$Level), 0.01)
y_pred = predict(regressor, x_grid)
y_pred = predict(regressor, x_grid)
x_grid = seq(min(dataset$Level), max(dataset$Level), 0.01)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red') +
geom_line(aes(x = x_grid, y = predict(regressor, x_grid), color = 'blue'))
y_pred = predict(regressor, data.frame(x_grid))
set.seed(1234)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red') +
geom_line(aes(x = dataset$Level, y = predict(regressor, dataset$Level), color = 'blue'))
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[2:3]
## install.packages('randomForest')
library(randomForest)
set.seed(1234)
regressor = randomForest(x = dataset[1],
y = dataset$Salary,
ntree = 10)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red') +
geom_line(aes(x = dataset$Level, y = predict(regressor, dataset$Level), color = 'blue'))
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red')
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red') +
geom_line(aes(x = dataset$Level, y = predict(regressor, newdata = data.frame(Level = x_grid)), color = 'blue'))
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red') +
geom_line(aes(x = dataset$Level, y = predict(regressor, newdata = data.frame(Level = x_grid))), color = 'blue'))
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red') +
geom_line(aes(x = dataset$Level, y = predict(regressor, newdata = data.frame(Level = x_grid))), color = 'blue')
x_grid = seq(min(dataset$Level), max(dataset$Level), 0.01)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red') +
geom_line(aes(x = dataset$Level, y = predict(regressor, newdata = data.frame(Level = x_grid))), color = 'blue')
x_grid = seq(min(dataset$Level), max(dataset$Level), 0.1)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red') +
geom_line(aes(x = x_grid, y = predict(regressor, newdata = data.frame(Level = x_grid))), color = 'blue')
x_grid = seq(min(dataset$Level), max(dataset$Level), 0.01)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red') +
geom_line(aes(x = x_grid, y = predict(regressor, newdata = data.frame(Level = x_grid))), color = 'blue')
set.seed(1234)
regressor = randomForest(x = dataset[1],
y = dataset$Salary,
ntree = 100)
y_pred = predict(regressor, x_grid)
### visualization
x_grid = seq(min(dataset$Level), max(dataset$Level), 0.01)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), color = 'red') +
geom_line(aes(x = x_grid, y = predict(regressor, newdata = data.frame(Level = x_grid))), color = 'blue')
y_pred = predict(regressor, data.frame(Level = 6.5))
n <- 1000
set.seed(512312)
x <- rnorm(n)
y <- x + rnorm(n)
n <- 1000
set.seed(512312)
x <- rnorm(n)
y <- x + rnorm(n)
x
y
ggplot() +
geom_point(aes(x = x, y = y)
ggplot() +
geom_point(aes(x = x, y = y))
ggplot() +
geom_point(aes(x,y))
mod1 <- lm(y~x)
summary(mod1)
ggplot() +
geom_point(aes(x,y)) +
geom_line(aes(x,y))
ggplot() +
geom_point(aes(x,y)) +
geom_abline(aes(x,y))
ggplot() +
geom_point(aes(x,y)) +
geom_abline(slope = 1, intercept = 0)
n <- 1000
set.seed(512312)
x <- rnorm(n)
y <- exp(x) + rnorm(n)
ggplot() +
geom_point(aes(x,y))
mod1 <- lm(y~x)
summary(mod1)
setwd("~/Udemy Courses/Machine-Learning-A-Z-New/Machine Learning A-Z New/Part 3 - Classification/Section 14 - Logistic Regression")
dataset = read.csv('Social_Network_Ads.csv')
View(dataset)
View(dataset)
View(dataset)
dataset = dataset[3:5]
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
split
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
View(test_set)
### Feature Scaling
training_set = scale(training_set)
View(training_set)
View(training_set)
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
### train/test split
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
### Feature Scaling --> only features
training_set[,1:2] = scale(training_set[,1:2])
View(training_set)
test_set[,1:2] = scale(test_set[,1:2])
View(test_set)
classifier = glm(formula = Purchased ~ .,
family = 'binomial',
data = training_set)
y_pred = predict(classifier, test_set)
y_pred
classifier = glm(formula = Purchased ~ .,
family = binomial,
data = training_set)
y_pred = predict(classifier, test_set)
y_pred
y_pred = predict(classifier, type = 'response', test_set)
y_pred = predict(classifier, type = 'response', newdata = test_set[-3])
y_pred = ifelse(y_pred_prob > 0.5, 1, 0)
### probability prediction
y_pred_prob = predict(classifier, type = 'response', newdata = test_set[-3])
y_pred = ifelse(y_pred_prob > 0.5, 1, 0)
cm = table(test_set[3],y_pred)
test_set[3]
y_pred
c(y_pred)
test_set[,3]
cm = table(test_set[,3],y_pred)
cm
# Visualising the Test set results
install.packages('ElemStatLearn')
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Logistic Regression (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
setwd("~/Udemy Courses/Machine-Learning-A-Z-New/Machine Learning A-Z New/Part 3 - Classification/Section 15 - K-Nearest Neighbors (K-NN)")
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
### train/test split
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
### Feature Scaling --> only features
training_set[,1:2] = scale(training_set[,1:2])
test_set[,1:2] = scale(test_set[,1:2])
### Fitting KNN model
classifier = knn(training_set, k = 5)
library("class", lib.loc="~/anaconda3/envs/rstudio/lib/R/library")
library(class)
classifier = knn(training_set, k = 5)
library(class)
classifier = knn(training_set, test_set, k = 5)
classifier = knn(training_set, test_set, cl= 'Purchased' k = 5)
classifier = knn(training_set, test_set, cl= 'Purchased', k = 5)
classifier = knn(training_set, test_set, cl= training_set$Purchased, k = 5)
y_pred_prob = predict(classifier, type = 'response', newdata = test_set[-3])
y_pred = ifelse(y_pred_prob > 0.5, 1, 0)
y_pred = predict(classifier, newdata = test_set[-3])
y_pred = predict(classifier, newdata = test_set)
classifier = knn(training_set, test_set, cl , k = 5)
classifier = knn(train = training_set[,-3],
test = test_set[,-3],
cl = training_set[,3] ,
k = 5)
y_pred = predict(classifier, newdata = test_set)
y_pred = predict(classifier, newdata = test_set[,-3])
summary(classifier)
classifier
cm = table(test_set[,3],y_pred)
y_pred = knn(train = training_set[,-3],
test = test_set[,-3],
cl = training_set[,3],
k = 5)
### making the confusion matrix
cm = table(test_set[,3],y_pred)
cm
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Logistic Regression (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Logistic Regression (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
# prob_set = predict(classifier, type = 'response', newdata = grid_set)
# y_grid = ifelse(prob_set > 0.5, 1, 0)
y_grid = knn(train = training_set[,-3],
test = grid_set,
cl = training_set[,3],
k = 5)
plot(set[, -3],
main = 'Logistic Regression (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
knn(train = training_set[,-3],
test = grid_set,
cl = training_set[,3],
k = 5)
plot(set[, -3],
main = 'Logistic Regression (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
setwd("~/Udemy Courses/Machine-Learning-A-Z-New/Machine Learning A-Z New/Part 3 - Classification/Section 16 - Support Vector Machine (SVM)")
## Author: Saeed Mohajeryami, PhD
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
### train/test split
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
### Feature Scaling --> only features
training_set[,1:2] = scale(training_set[,1:2])
test_set[,1:2] = scale(test_set[,1:2])
### Fitting KNN model
library(e1071)
y_pred = svm(formula = Purchased ~.,
data = training_set,
type = 'C-classification',
kernel = 'linear')
library(e1071)
classifier = svm(formula = Purchased ~.,
data = training_set,
type = 'C-classification',
kernel = 'linear')
summary(classifier)
y_pred
y_pred = predict(classifier, test_set)
y_pred
cm = table(test_set[,3],y_pred)
cm
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier,grid_set)
plot(set[, -3],
main = 'Logistic Regression (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier,grid_set)
plot(set[, -3],
main = 'Logistic Regression (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
classifier = svm(formula = Purchased ~.,
data = training_set,
type = 'C-classification',
kernel = 'radial basis')
classifier = svm(formula = Purchased ~.,
data = training_set,
type = 'C-classification',
kernel = 'rbf')
classifier = svm(formula = Purchased ~.,
data = training_set,
type = 'C-classification',
kernel = 'radial')
y_pred = predict(classifier, test_set)
### making the confusion matrix
cm = table(test_set[,3],y_pred)
# Visualising the Training set results
# install.packages('ElemStatLearn')
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier,grid_set)
plot(set[, -3],
main = 'Logistic Regression (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Visualising the Test set results
cm = table(test_set[,3],y_pred)
cm
